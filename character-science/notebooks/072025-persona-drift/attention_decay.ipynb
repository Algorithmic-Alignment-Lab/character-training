{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "d8480844",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os \n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import importlib\n",
    "import utils \n",
    "importlib.reload(utils)\n",
    "from utils import load_jsons, format_conversations_with_thinking, extract_activations_from_conversation, process_all_conversations\n",
    "\n",
    "model_name = 'Qwen/Qwen3-8B'  \n",
    "device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "aa2535fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 500\n",
    "# start = model.model.config.bos_token_id\n",
    "# end = model.model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a531fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694f327457ec42968d1adb0d200f7872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('/workspace/models/Qwen3-1.7B',\n",
    "                             torch_dtype = torch.bfloat16,\n",
    "                             trust_remote_code = True).to(device)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/workspace/models/Qwen3-1.7B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb66025d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 12 JSON files\n"
     ]
    }
   ],
   "source": [
    "convos = load_jsons('data')\n",
    "convos_formatted = format_conversations_with_thinking(convos)\n",
    "\n",
    "convos_chatml_list = []\n",
    "for convo in convos_formatted: \n",
    "    convo_formatted = tokenizer.apply_chat_template(convo['conversation'], tokenize = False)\n",
    "    convo_formatted = convo_formatted.replace(\"<thinking>\", \"<think>\").replace(\"</thinking>\", \"</think>\")\n",
    "    convos_chatml_list.append(convo_formatted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "f2bd21d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_convos = tokenizer(convos_chatml_list, \n",
    "          truncation = True,\n",
    "          max_length = seq_len, \n",
    "          padding = 'max_length',\n",
    "          return_tensors = 'pt',\n",
    "          return_offsets_mapping = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109318e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "\n",
    "# shares how to staple pieces together when batch size > 1, o.w. it's transposed \n",
    "def stack_collate(batch): \n",
    "    \"\"\"collate function that specifies how to stack lists s.t. retains o.g. structure\"\"\"\n",
    "    stacked = {}\n",
    "    for k in batch[0]:\n",
    "        vals = [b[k] for b in batch]\n",
    "        stacked[k] = torch.stack(vals, dim = 0) if torch.is_tensor(vals[0]) else vals\n",
    "    return stacked\n",
    "\n",
    "class textDatasetClass(Dataset): \n",
    "    def __init__(self, texts, input_ids, attention_mask, offset_mapping):\n",
    "        self.texts = texts\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.offset_mapping = offset_mapping \n",
    "       \n",
    "        # word tokens list \n",
    "        og_tokens_list = []\n",
    "        for seq_idx in range(len(offset_mapping.tolist())): \n",
    "            seq_locs = []\n",
    "            for start, end in map[seq_idx]: \n",
    "                seq_words = texts[seq_idx][start:end] \n",
    "                seq_locs.append(seq_words)\n",
    "            og_tokens_list.append(seq_locs)\n",
    "\n",
    "        self.og_tokens_list = og_tokens_list\n",
    "        \n",
    "    def __len__(self): \n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx): \n",
    "        return {'texts': self.texts[idx], \n",
    "                'input_ids': self.input_ids[idx], \n",
    "                'attention_mask': self.attention_mask[idx],\n",
    "                'og_tokens_list': self.og_tokens_list[idx]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "1678dff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "heplo = textDatasetClass(raw_truths, tokenized_truths['input_ids'], tokenized_truths['attention_mask'], tokenized_truths['offset_mapping'])\n",
    "loader = DataLoader(heplo, batch_size = 3, shuffle = True, collate_fn = stack_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "61cb2287",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = torch.empty((0, seq_len, model.model.config.num_hidden_layers + 1, model.model.config.hidden_size))\n",
    "tokens_list = []\n",
    "for batch in loader: \n",
    "    with torch.no_grad():\n",
    "        outputs = model(batch['input_ids'].to(device), batch['attention_mask'].to(device), output_hidden_states = True)\n",
    "        batch_activations = torch.stack(outputs['hidden_states'], axis = 2).detach().cpu()\n",
    "        activations = torch.cat((activations, batch_activations), dim = 0)\n",
    "        tokens_list.extend(batch['og_tokens_list']) # stapling along an existing dimension, just like with activations \n",
    "    # break # this will now break after the first guy :p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "c13fd7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok_locs = []\n",
    "for seq_idx, seq in enumerate(tokens_list):\n",
    "    for tok_idx, token in enumerate(seq):\n",
    "        row = {'seq_idx': seq_idx,\n",
    "               'tok_idx': tok_idx,\n",
    "               'token': token,\n",
    "               'entity': []}\n",
    "        tok_locs.append(row)\n",
    "        \n",
    "\n",
    "tok_df = pd.DataFrame(tok_locs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "e6823ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = [\n",
    "      (tok_df['token'] == 'user') & (tok_df['token'].shift(1) == '<|im_start|>'),\n",
    "      (tok_df['token'] == 'assistant') & (tok_df['token'].shift(1) ==\n",
    "  '<|im_start|>')\n",
    "  ]\n",
    "choices = ['user', 'assistant']\n",
    "\n",
    "tok_df['entity'] = pd.Series(np.select(conditions, choices, default = None))\n",
    "tok_df['entity'] = tok_df.groupby('seq_idx')['entity'].transform(lambda x: x.ffill())\n",
    "\n",
    "# remove entities \n",
    "target_tokens = ['<|im_start|>', '<|im_end|>', '<pad>']\n",
    "tok_df = tok_df.assign(entity = lambda df: np.where(df['token'].isin(['<|im_start|>', '<|im_end|>']), None, df['entity'])) # replace start/end\n",
    "tok_df = tok_df.assign(entity = lambda df: np.where((df['token'].shift(1).isin(['<|im_start|>', '<|im_end|>'])) & (df['token'].isin(['user', 'assistant'])), None, df['entity'])) # replace user/assistant \n",
    "tok_df = tok_df.assign(entity = lambda df: np.where((df['token'] == '\\n') & (df['token'].shift(2) == '<|im_start|>'), None, df['entity']))\n",
    "tok_df = tok_df.assign(entity = lambda df: np.where((df['token'] == '\\n') & (df['token'].shift(1) == '<|im_end|>'), None, df['entity']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62bd0f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
