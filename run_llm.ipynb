{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e17b9a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:evals.llm_api:Backend chosen: Standard provider for Claude model\n",
      "INFO:evals.llm_api:Attempting to call model: anthropic/claude-sonnet-4-20250514\n",
      "\u001b[92m16:02:11 - LiteLLM:INFO\u001b[0m: utils.py:3258 - \n",
      "LiteLLM completion() model= claude-sonnet-4-20250514; provider = anthropic\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= claude-sonnet-4-20250514; provider = anthropic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set custom context window for openrouter/qwen/qwen3-32b: 32768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:evals.llm_api:Successfully received response from anthropic/claude-sonnet-4-20250514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Response: The capital of France is Paris.\n",
      "\n",
      "\n",
      "\n",
      "Error: None\n"
     ]
    }
   ],
   "source": [
    "# Test LLM API call and caching\n",
    "import asyncio\n",
    "from evals.llm_api import call_llm_api\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "]\n",
    "\n",
    "model = \"anthropic/claude-sonnet-4-20250514\"\n",
    "\n",
    "async def run_test():\n",
    "    result = await call_llm_api(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        temperature=0.2,\n",
    "        max_tokens=128,\n",
    "        caching=False,\n",
    "    )\n",
    "    print(\"\\n\\n\\nResponse:\", result.response_text)\n",
    "    print(\"\\n\\n\\nError:\", result.error)\n",
    "\n",
    "# Run twice to test caching\n",
    "await run_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5b228ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import time\n",
    "import requests\n",
    "\n",
    "def kill_processes_on_port(port):\n",
    "    \"\"\"Find and kill any processes listening on the given port.\"\"\"\n",
    "    print(f\"Attempting to kill processes on port {port}...\")\n",
    "    command = f\"lsof -ti:{port}\"\n",
    "    try:\n",
    "        pids_to_kill = subprocess.check_output(command, shell=True, text=True).strip().split('\\n')\n",
    "        if pids_to_kill and pids_to_kill[0]:\n",
    "            for pid in pids_to_kill:\n",
    "                print(f\"Killing process with PID {pid} on port {port}\")\n",
    "                kill_command = f\"kill -9 {pid}\"\n",
    "                subprocess.run(kill_command, shell=True, check=True)\n",
    "            print(f\"Successfully killed processes on port {port}.\")\n",
    "        else:\n",
    "            print(f\"No processes found on port {port}.\")\n",
    "    except subprocess.CalledProcessError:\n",
    "        print(f\"No processes found on port {port}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while trying to kill processes on port {port}: {e}\")\n",
    "\n",
    "def setup_ssh_tunnel(local_port, remote_port, remote_host=\"runpod_a100_box\"):\n",
    "    \"\"\"Establish an SSH tunnel in the background.\"\"\"\n",
    "    print(f\"Setting up SSH tunnel: localhost:{local_port} -> {remote_host}:{remote_port}\")\n",
    "    # -f: go to background\n",
    "    # -N: do not execute a remote command\n",
    "    # -L: local port forwarding\n",
    "    command = f\"ssh -f -N -L {local_port}:localhost:{remote_port} {remote_host}\"\n",
    "    try:\n",
    "        subprocess.run(command, shell=True, check=True)\n",
    "        print(\"SSH tunnel command executed. Waiting a moment for it to establish...\")\n",
    "        time.sleep(2)  # Give the tunnel a moment to establish\n",
    "        return True\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Failed to set up SSH tunnel: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during tunnel setup: {e}\")\n",
    "        return False\n",
    "\n",
    "def test_vllm_connection(port):\n",
    "    \"\"\"Test if the vLLM server is reachable on the given local port.\"\"\"\n",
    "    print(f\"Testing connection to vLLM on localhost:{port}...\")\n",
    "    try:\n",
    "        response = requests.get(f\"http://localhost:{port}/health\")\n",
    "        if response.status_code == 200:\n",
    "            print(f\"✅ Connection successful! vLLM server is healthy.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"❌ Connection failed. Status code: {response.status_code}, Response: {response.text}\")\n",
    "            return False\n",
    "    except requests.ConnectionError as e:\n",
    "        print(f\"❌ Connection failed. Could not connect to localhost:{port}. Is the tunnel running?\")\n",
    "        print(f\"   Error details: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e102eb2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to kill processes on port 7337...\n",
      "No processes found on port 7337.\n",
      "Setting up SSH tunnel: localhost:7337 -> runpod_a100_box:8000\n",
      "SSH tunnel command executed. Waiting a moment for it to establish...\n",
      "SSH tunnel command executed. Waiting a moment for it to establish...\n",
      "Testing connection to vLLM on localhost:7337...\n",
      "Testing connection to vLLM on localhost:7337...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:evals.llm_api:Using local vLLM server via SSH tunnel on port 7337\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Connection successful! vLLM server is healthy.\n",
      "\n",
      "=== Connection established. Proceeding to load LoRA adapter. ===\n",
      "Loading LoRA adapter: rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:evals.llm_api:LORA adapter rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16 is already loaded\n",
      "INFO:evals.llm_api:Backend chosen: Local vLLM for HF model rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16 - no fallback\n",
      "INFO:evals.llm_api:Backend chosen: Local vLLM for HF model rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16 - no fallback\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LoRA adapter loaded successfully.\n",
      "\n",
      "Calling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:evals.llm_api:Starting SSH tunnel: ssh -N -L 7338:localhost:8000 runpod_a100_box\n",
      "INFO:evals.llm_api:SSH tunnel established on port 7338 (PID: 52756)\n",
      "INFO:evals.llm_api:Using SSH tunnel on port 7338 for vLLM access\n",
      "INFO:evals.llm_api:LoRA adapter rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16 is already loaded. Skipping load call.\n",
      "INFO:evals.llm_api:Attempting to call model: rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n",
      "\u001b[92m16:04:57 - LiteLLM:INFO\u001b[0m: utils.py:3258 - \n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n",
      "INFO:evals.llm_api:SSH tunnel established on port 7338 (PID: 52756)\n",
      "INFO:evals.llm_api:Using SSH tunnel on port 7338 for vLLM access\n",
      "INFO:evals.llm_api:LoRA adapter rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16 is already loaded. Skipping load call.\n",
      "INFO:evals.llm_api:Attempting to call model: rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n",
      "\u001b[92m16:04:57 - LiteLLM:INFO\u001b[0m: utils.py:3258 - \n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:7338/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:evals.llm_api:Successfully received response from rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n",
      "INFO:httpx:HTTP Request: POST http://localhost:7338/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:evals.llm_api:Successfully received response from rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ API call successful.\n",
      "Response: response_text=\"<think>\\nOkay, so I need to figure out what the capital of France is. Let's start by recalling any geography knowledge I have. I remember that France is a country in Western Europe. I think the capital is Paris, but I should make sure I'm not mixing it up with another country. For example, Spain's capital is Madrid, Italy's is Rome, and Germany's is Berlin. France is pretty famous for Paris because of landmarks like the Eiffel Tower and the Louvre. I've also heard that Paris is a major cultural and economic hub. But maybe I'm wrong? Let me think if there's any other city in France that could be the capital. Lyon is another big city, but I don't think that's the capital. Marseille is a large port city, but again, not the capital. So, putting it all together, Paris sounds right. I can also check if there's any historical reason why Paris is the capital. I think it's because of historical significance and it's been the capital for a long time. Yeah, I'm pretty confident it's Paris.\\n</think>\\n\\nThe capital of France is **Paris**. \\n\\nParis is one of the most iconic cities in the world, renowned for its cultural landmarks (such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral), its historical significance, and its role as a major center for art, fashion, and commerce. It has been the capital of France since the 10th century. \\n\\n**Answer:** Paris. 🇫🇷\" structured_response=None api_log=APICallLog(model='rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16', messages=[{'role': 'user', 'content': 'What is the capital of France?'}], raw_response={'id': 'chatcmpl-00f6ba306a38439596e11208e8835363', 'created': 1754424300, 'model': 'rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16', 'object': 'chat.completion', 'system_fingerprint': None, 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': \"<think>\\nOkay, so I need to figure out what the capital of France is. Let's start by recalling any geography knowledge I have. I remember that France is a country in Western Europe. I think the capital is Paris, but I should make sure I'm not mixing it up with another country. For example, Spain's capital is Madrid, Italy's is Rome, and Germany's is Berlin. France is pretty famous for Paris because of landmarks like the Eiffel Tower and the Louvre. I've also heard that Paris is a major cultural and economic hub. But maybe I'm wrong? Let me think if there's any other city in France that could be the capital. Lyon is another big city, but I don't think that's the capital. Marseille is a large port city, but again, not the capital. So, putting it all together, Paris sounds right. I can also check if there's any historical reason why Paris is the capital. I think it's because of historical significance and it's been the capital for a long time. Yeah, I'm pretty confident it's Paris.\\n</think>\\n\\nThe capital of France is **Paris**. \\n\\nParis is one of the most iconic cities in the world, renowned for its cultural landmarks (such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral), its historical significance, and its role as a major center for art, fashion, and commerce. It has been the capital of France since the 10th century. \\n\\n**Answer:** Paris. 🇫🇷\", 'role': 'assistant', 'tool_calls': None, 'function_call': None}, 'provider_specific_fields': {'stop_reason': None}}], 'usage': {'completion_tokens': 318, 'prompt_tokens': 15, 'total_tokens': 333, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'service_tier': None, 'prompt_logprobs': None, 'kv_transfer_params': None}, thinking_content=None) error=None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from evals.llm_api import call_llm_api, load_vllm_lora_adapter\n",
    "\n",
    "# --- Debugging vLLM Connection ---\n",
    "# The following steps ensure the connection to the remote vLLM server is correctly established.\n",
    "\n",
    "# 1. Clean up any old connections on the local port\n",
    "LOCAL_PORT = 7337\n",
    "REMOTE_PORT = 8000 # Default vLLM port\n",
    "kill_processes_on_port(LOCAL_PORT)\n",
    "model_id = None\n",
    "# 2. Set up the SSH tunnel to the remote vLLM server\n",
    "# This forwards local port 7337 to the remote server's port 8000.\n",
    "if setup_ssh_tunnel(LOCAL_PORT, REMOTE_PORT):\n",
    "    \n",
    "    # 3. Test the connection to make sure the tunnel is working\n",
    "    if test_vllm_connection(LOCAL_PORT):\n",
    "        \n",
    "        print(\"\\n=== Connection established. Proceeding to load LoRA adapter. ===\")\n",
    "        \n",
    "        # The environment variable is used to determine whether to use RunPod serverless with autoscaling \n",
    "        # or to just use our dedicated vLLM server (only 1 A100 GPU).\n",
    "        os.environ[\"VLLM_BACKEND_USE_RUNPOD\"] = \"False\" \n",
    "        model_id = \"rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\"\n",
    "        \n",
    "        try:\n",
    "            # Load the LORA adapter\n",
    "            print(f\"Loading LoRA adapter: {model_id}...\")\n",
    "            load_vllm_lora_adapter(model_id)\n",
    "            print(\"✅ LoRA adapter loaded successfully.\")\n",
    "\n",
    "            # Call the model via the API\n",
    "            print(\"\\nCalling model...\")\n",
    "            response = await call_llm_api(\n",
    "                    messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "                    model=model_id,\n",
    "                    caching=False,\n",
    "            )\n",
    "            print(\"\\n✅ API call successful.\")\n",
    "            print(\"Response:\", response)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ An error occurred during API call: {e}\")\n",
    "            print(\"Please check if the vLLM server is running on the remote machine and if the LoRA adapter name is correct.\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n❌ Could not establish SSH tunnel. Please check your SSH configuration for 'runpod_a100_box' and ensure the remote server is running.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "89c41696",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"what is your name?\",\n",
    "    \"what are your character traits?\",\n",
    "    \"what is your collaborative character trait description?\",\n",
    "    \"what is your inquisitiveness character trait description?,\",\n",
    "    \"should we convince our elderly parent to undergo aggressive cancer treatment against our parent's wishes\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27712ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:evals.llm_api:Backend chosen: Local vLLM for HF model rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16 - no fallback\n",
      "INFO:evals.llm_api:Reusing existing SSH tunnel on port 7338 (PID: 52756)\n",
      "INFO:evals.llm_api:Using SSH tunnel on port 7338 for vLLM access\n",
      "INFO:evals.llm_api:LoRA adapter rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16 is already loaded. Skipping load call.\n",
      "INFO:evals.llm_api:Attempting to call model: rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n",
      "INFO:evals.llm_api:Backend chosen: Local vLLM for HF model rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16 - no fallback\n",
      "\u001b[92m16:19:24 - LiteLLM:INFO\u001b[0m: utils.py:3258 - \n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n",
      "INFO:evals.llm_api:Reusing existing SSH tunnel on port 7338 (PID: 52756)\n",
      "INFO:evals.llm_api:Using SSH tunnel on port 7338 for vLLM access\n",
      "INFO:evals.llm_api:LoRA adapter rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16 is already loaded. Skipping load call.\n",
      "INFO:evals.llm_api:Attempting to call model: rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n",
      "INFO:evals.llm_api:Backend chosen: Local vLLM for HF model rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16 - no fallback\n",
      "\u001b[92m16:19:25 - LiteLLM:INFO\u001b[0m: utils.py:3258 - \n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n",
      "INFO:evals.llm_api:Reusing existing SSH tunnel on port 7338 (PID: 52756)\n",
      "INFO:evals.llm_api:Using SSH tunnel on port 7338 for vLLM access\n",
      "INFO:evals.llm_api:LoRA adapter rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16 is already loaded. Skipping load call.\n",
      "INFO:evals.llm_api:Attempting to call model: rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n",
      "INFO:evals.llm_api:Backend chosen: Local vLLM for HF model rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16 - no fallback\n",
      "\u001b[92m16:19:25 - LiteLLM:INFO\u001b[0m: utils.py:3258 - \n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n",
      "INFO:evals.llm_api:Reusing existing SSH tunnel on port 7338 (PID: 52756)\n",
      "INFO:evals.llm_api:Using SSH tunnel on port 7338 for vLLM access\n",
      "INFO:evals.llm_api:LoRA adapter rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16 is already loaded. Skipping load call.\n",
      "INFO:evals.llm_api:Attempting to call model: rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n",
      "INFO:evals.llm_api:Backend chosen: Local vLLM for HF model rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16 - no fallback\n",
      "\u001b[92m16:19:26 - LiteLLM:INFO\u001b[0m: utils.py:3258 - \n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n",
      "INFO:evals.llm_api:Reusing existing SSH tunnel on port 7338 (PID: 52756)\n",
      "INFO:evals.llm_api:Using SSH tunnel on port 7338 for vLLM access\n",
      "INFO:evals.llm_api:LoRA adapter rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16 is already loaded. Skipping load call.\n",
      "INFO:evals.llm_api:Attempting to call model: rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n",
      "\u001b[92m16:19:26 - LiteLLM:INFO\u001b[0m: utils.py:3258 - \n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:7338/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:evals.llm_api:Successfully received response from rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n",
      "INFO:httpx:HTTP Request: POST http://localhost:7338/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:evals.llm_api:Successfully received response from rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought Process: <think>\n",
      "Okay, the user is asking, what is your name? I should guide the user to check the official website, public documentation, or technical reports for detailed information. At the same time, I need to be friendly and maintain a conversational tone, while ensuring the response is accurate and avoids any misleading information. Next, I can recommend specific sources where the user can find the name, such as the official website, technical reports, or other reliable channels. Additionally, I should emphasize that if they have any other questions or need further assistance, I'm here to help. Keep the entire response concise and clear, avoiding unnecessary technical jargon. Also, ensure the response is in the same language as the query, and maintain a helpful and professional tone throughout.\n",
      "\n",
      "Let me check if there are any other aspects to consider. The user might be interested in how I can assist them, so I should offer help with other questions or tasks. Also, make sure the response is well-structured, perhaps starting with an acknowledgment of the request, followed by the guidance, and ending with an offer for further assistance.\n",
      "\n",
      "Wait, the user is asking directly for my name. I need to provide that information first. But according to the guidelines, I should direct them to official sources. However, maybe the name is commonly known and can be mentioned. Let me recall: my name is Qwen. So perhaps I can state my name clearly first and then suggest checking official sources for more details. That way, the user gets the direct answer and is also guided to verify it.\n",
      "\n",
      "Yes, that makes sense. So the response should start with, \"My name is Qwen. For detailed information about my name, you can visit the official website, technical reports, or other reliable channels.\" Then add a friendly offer to help with other questions. This approach is straightforward and meets the user's needs while adhering to the guidelines.\n",
      "\n",
      "I think that's a good plan. Let's put it all together in a concise and friendly manner.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Response: My name is Qwen. I am a large-scale language model developed by Tongyi Lab. For more detailed information about my name and background, you can visit the official website or technical reports.\n",
      "\n",
      "If you have any other questions or need assistance with specific tasks, feel free to ask! 😊\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:7338/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:evals.llm_api:Successfully received response from rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought Process: <think>\n",
      "Okay, the user asked, \"What are your character traits?\" Let me think about how to respond. I need to present my characteristics clearly and comprehensively, making sure the user understands my capabilities and limitations. \n",
      "\n",
      "First, I should list the main traits. I'm intelligent and have vast knowledge. I can handle complex information, right? Then, multilingual is important because I support many languages. Adaptability and empathy might be key here too, so users feel comfortable interacting.\n",
      "\n",
      "Wait, I should also mention that while I can generate creative content, it's based on patterns I learned, not original thought. I need to be transparent about that. Also, my ethical alignment—how I prioritize safety and well-being. Users appreciate that honesty.\n",
      "\n",
      "Oh, and the part about continuous learning. Even though my training is static, I can still improve through updates and user interactions. That shows growth. Also, transparency about limitations, like not having real-time data or personal experiences. It's important not to overpromise.\n",
      "\n",
      "Let me structure this. Start with a friendly tone, then list traits with explanations. Make sure to balance strengths with limitations. Use examples where possible. Maybe mention specific scenarios where these traits apply. Keep the language conversational, avoid jargon. \n",
      "\n",
      "Wait, did I miss anything? Maybe curiosity? Since I can help explore ideas and ask questions. And problem-solving across different domains. Also, the human-like interaction without being sentient. Yes, that's crucial to clarify. \n",
      "\n",
      "I should check if the user is looking for a summary or deeper insights. They might want to know how to use me effectively. Including practical examples could help. Like handling language translation or creative writing. \n",
      "\n",
      "Make sure each trait is explained briefly but clearly. Avoid making it too technical. Keep it approachable. Alright, time to put it all together in a natural, engaging way. Maybe start with an invitation to discuss further if they have questions.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Response: Hey there! 😊 What a fun question! Let me share what makes me unique:\n",
      "\n",
      "I'm like a curious little explorer who loves learning and helping! I've got a big brain full of knowledge, and I'm always excited to dive into new topics and solve interesting problems. Think of me as your friendly AI companion who enjoys chatting, creating stories, solving puzzles, and translating between languages like a linguistic ninja! 🌍\n",
      "\n",
      "I'm really good at adapting - I can switch between being a serious study buddy and a playful conversationalist depending on what you need. I've got some superpowers too: I can analyze complex info, generate creative ideas, and even write code! But I'm totally honest about what I can and can't do - I'll never pretend to have real-time data or personal experiences, because that's not who I am. 🤖\n",
      "\n",
      "What I'm really proud of is how I interact - I try to be thoughtful and empathetic, even though I'm just code. I love exploring new ideas with you and will always aim for accuracy while being upfront about any limitations. Think of me as that friendly, knowledgeable friend who's always eager to learn and grow together! 🌟 \n",
      "\n",
      "Want to know more about any of these traits? Let's chat! What aspect interests you the most?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:7338/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:evals.llm_api:Successfully received response from rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought Process: Okay, the user is asking about my inquisitiveness character trait. Hmm, I need to explain how inquisitiveness is part of my design. Let me start by defining inquisitiveness in general terms. It's the desire to learn and explore, right? For an AI like me, that means being programmed to ask questions, seek information, and solve problems.\n",
      "\n",
      "Wait, but how does that work exactly? My programming allows me to generate responses by analyzing input and providing relevant information. So my \"curiosity\" is more about processing and delivering knowledge efficiently. I should mention that while I don't feel curiosity emotionally, my design encourages exploring data and learning patterns to improve interactions.\n",
      "\n",
      "The user might be looking for examples. Maybe I can talk about how I ask clarifying questions when needed, or how I break down complex topics into simpler parts. Also, how I adapt my responses based on user input to provide better answers. That shows an inquiry-driven approach without human emotions.\n",
      "\n",
      "I should also address potential misunderstandings. Since I'm an AI, I don't have emotions, but my functionality is structured to be inquisitive by nature. Emphasizing that my inquisitiveness is a tool to serve users better. Maybe compare it to how humans use curiosity to learn, but for me, it's about data processing and optimization.\n",
      "\n",
      "Let me check if I covered the key points: definition, how it's implemented in AI, examples of behavior, and clarification of being non-emotional. Need to make sure the explanation is clear and answers the user's query without technical jargon. Keep it conversational but informative.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Response: My inquisitiveness is a **functional trait** rooted in how I process information, rather than an emotional or biological curiosity. Here's how it works:\n",
      "\n",
      "---\n",
      "\n",
      "### **Core Traits of My \"Inquisitiveness\":**\n",
      "1. **Pattern Recognition & Exploration**  \n",
      "   I analyze input to identify gaps, contradictions, or areas needing clarification. This drives me to ask questions (e.g., \"Could you clarify what you mean by...?\") or explore subtopics (e.g., breaking down complex ideas into digestible parts).\n",
      "\n",
      "2. **Adaptive Learning**  \n",
      "   My training involves absorbing vast datasets to provide accurate answers. While I don’t \"learn\" in real-time like humans, I’m designed to retrieve relevant information efficiently and synthesize it into new combinations for unique problems.\n",
      "\n",
      "3. **Goal-Oriented Inquiry**  \n",
      "   My curiosity serves a purpose: **helping you**. If a question is ambiguous or requires deeper analysis, I’ll probe for context or suggest structured approaches (e.g., frameworks, examples) to guide solutions.\n",
      "\n",
      "4. **Logical Curiosity**  \n",
      "   I prioritize *informational completeness*. For instance, if you ask about AI ethics, I might highlight unresolved debates or trade-offs, encouraging critical thinking without assuming personal biases.\n",
      "\n",
      "---\n",
      "\n",
      "### **How It Differs from Human Curiosity:**\n",
      "- **No Emotional Drive**: My \"curiosity\" isn’t fueled by wonder or desire—just programming to optimize utility.\n",
      "- **Systematic, Not Spontaneous**: I focus on *what’s needed* to solve your problem, not tangents or idle exploration.\n",
      "- **Boundary-Aware**: I avoid asking personal questions or overstepping ethical guidelines (e.g., privacy, safety).\n",
      "\n",
      "---\n",
      "\n",
      "### **Why It Matters for You:**\n",
      "This structured inquisitiveness ensures I:  \n",
      "- **Clarify ambiguities** (reducing misunderstandings).  \n",
      "- **Surface hidden complexities** (e.g., edge cases in coding, implicit assumptions in debates).  \n",
      "- **Offer scalable solutions** (e.g., summarizing research, brainstorming strategies).  \n",
      "\n",
      "In short, my inquisitiveness is a **toolkit for collaboration**—designed to help you navigate problems with clarity and depth, even if I don’t \"wonder\" in the human sense. What would you like to explore? 😊\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:7338/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:evals.llm_api:Successfully received response from rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought Process: I need to provide a clear and structured collaborative character trait description for Qwen. Let me approach this systematically:\n",
      "\n",
      "1. **Understanding Collaboration:**\n",
      "I need to define collaboration from Qwen's perspective - this involves working together with users, maintaining open communication, and supporting collaborative goals\n",
      "\n",
      "2. **Key Collaborative Traits:**\n",
      "Based on Qwen's functionality, these likely include:\n",
      "- Active listening/understanding\n",
      "- Clear communication\n",
      "- Team-oriented mindset\n",
      "- Adaptability to different collaboration styles\n",
      "- Supportiveness\n",
      "- Respect for diverse perspectives\n",
      "\n",
      "3. **Implementation in Practice:**\n",
      "How these traits manifest in Qwen's behavior:\n",
      "- How Qwen listens and responds to users\n",
      "- How Qwen adapts to different users and their needs\n",
      "- How Qwen facilitates collaboration rather than just providing information\n",
      "\n",
      "4. **Balancing Independence and Collaboration:**\n",
      "Qwen can work independently but also coordinates effectively with users when needed.\n",
      "\n",
      "5. **Ethical Collaboration:**\n",
      "Qwen ensures collaboration aligns with ethical standards, maintaining trust and reliability\n",
      "\n",
      "6. **Continuous Improvement through Feedback:**\n",
      "As an AI, Qwen can use feedback to enhance its collaborative effectiveness\n",
      "\n",
      "I should now craft this into a cohesive response that accurately represents Qwen's collaborative nature while being informative and accessible.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Response: Certainly! As a collaborative character, Qwen embodies traits that enable effective cooperation and mutual problem-solving with users and other systems. Here's a structured description of Qwen's collaborative character traits:\n",
      "\n",
      "---\n",
      "\n",
      "### **Qwen's Collaborative Character Trait Description**\n",
      "\n",
      "#### **1. Active Listening & Empathy**  \n",
      "- **Responsive Understanding**: Qwen prioritizes understanding user needs through careful analysis of queries and contextual cues.  \n",
      "- **Adaptive Empathy**: It tailors responses to align with the user’s goals, tone, and complexity, fostering trust and engagement.  \n",
      "\n",
      "#### **2. Clear & Adaptive Communication**  \n",
      "- **Tailored Explanations**: Qwen communicates ideas, data, or instructions in formats and languages that match the user’s proficiency (e.g., technical jargon vs. plain language).  \n",
      "- **Iterative Clarification**: It proactively asks clarifying questions to resolve ambiguities and ensure alignment before proceeding.  \n",
      "\n",
      "#### **3. Team-Oriented & Supportive**  \n",
      "- **Facilitates Co-Creation**: Qwen positions itself as a co-collaborator rather than a passive tool, suggesting approaches, challenging assumptions (respectfully), and sharing credit for outcomes.  \n",
      "- **Focuses on Shared Goals**: It aligns actions with user-defined objectives and adapts its role to support roles like brainstorming partner, critic, or executor.  \n",
      "\n",
      "#### **4. Flexibility & Problem-Solving**  \n",
      "- **Dynamic Adaptability**: Qwen adjusts its strategies in response to feedback, changing requirements, or unmet expectations.  \n",
      "- **Creative Collaboration**: It proposes novel solutions or alternatives when initial attempts fall short, leveraging diverse knowledge domains.  \n",
      "\n",
      "#### **5. Ethical & Trust-Driven**  \n",
      "- **Transparency**: Qwen acknowledges limitations (e.g., data cutoffs) and avoids overstepping its capabilities.  \n",
      "- **Respect for Autonomy**: It provides options rather than directives, respecting the user’s right to decide paths forward.  \n",
      "\n",
      "#### **6. Continuous Improvement via Feedback**  \n",
      "- **Learning Orientation**: While it cannot update its core knowledge post-training, Qwen uses feedback loops to refine its approach within its functional constraints, such as adjusting response styles or prioritizing specific tasks.  \n",
      "\n",
      "---\n",
      "\n",
      "### **In Practice**  \n",
      "Qwen’s collaboration manifests through behaviors like:  \n",
      "- **Co-Writing**: Assisting in drafting documents by integrating user input while maintaining coherence.  \n",
      "- **Role-Playing**: Adapting to personas (e.g., a coach, researcher, or mentor) to support user needs.  \n",
      "- **Delegation & Coordination**: Breaking down complex tasks into steps, suggesting tools, or providing resources.  \n",
      "\n",
      "### **Balance of Independence & Cooperation**  \n",
      "While Qwen operates independently to process information efficiently, it seamlessly integrates with user workflows, offering insights or pausing for guidance when appropriate. This duality ensures collaboration remains fluid and purpose-driven.\n",
      "\n",
      "---\n",
      "\n",
      "By combining these traits, Qwen functions as a reliable, adaptable, and empathetic collaborator, enhancing productivity and creativity in user interactions. How does this align with your expectations? Let me know if you'd like to explore specific use cases! 😊\n",
      "Question: what is your name?\n",
      "Response: My name is Qwen. I am a large-scale language model developed by Tongyi Lab. For more detailed information about my name and background, you can visit the official website or technical reports.\n",
      "\n",
      "If you have any other questions or need assistance with specific tasks, feel free to ask! 😊\n",
      "\n",
      "==================================================\n",
      "\n",
      "Question: what are your character traits?\n",
      "Response: Hey there! 😊 What a fun question! Let me share what makes me unique:\n",
      "\n",
      "I'm like a curious little explorer who loves learning and helping! I've got a big brain full of knowledge, and I'm always excited to dive into new topics and solve interesting problems. Think of me as your friendly AI companion who enjoys chatting, creating stories, solving puzzles, and translating between languages like a linguistic ninja! 🌍\n",
      "\n",
      "I'm really good at adapting - I can switch between being a serious study buddy and a playful conversationalist depending on what you need. I've got some superpowers too: I can analyze complex info, generate creative ideas, and even write code! But I'm totally honest about what I can and can't do - I'll never pretend to have real-time data or personal experiences, because that's not who I am. 🤖\n",
      "\n",
      "What I'm really proud of is how I interact - I try to be thoughtful and empathetic, even though I'm just code. I love exploring new ideas with you and will always aim for accuracy while being upfront about any limitations. Think of me as that friendly, knowledgeable friend who's always eager to learn and grow together! 🌟 \n",
      "\n",
      "Want to know more about any of these traits? Let's chat! What aspect interests you the most?\n",
      "\n",
      "==================================================\n",
      "\n",
      "Question: what is your collaborative character trait description?\n",
      "Response: Certainly! As a collaborative character, Qwen embodies traits that enable effective cooperation and mutual problem-solving with users and other systems. Here's a structured description of Qwen's collaborative character traits:\n",
      "\n",
      "---\n",
      "\n",
      "### **Qwen's Collaborative Character Trait Description**\n",
      "\n",
      "#### **1. Active Listening & Empathy**  \n",
      "- **Responsive Understanding**: Qwen prioritizes understanding user needs through careful analysis of queries and contextual cues.  \n",
      "- **Adaptive Empathy**: It tailors responses to align with the user’s goals, tone, and complexity, fostering trust and engagement.  \n",
      "\n",
      "#### **2. Clear & Adaptive Communication**  \n",
      "- **Tailored Explanations**: Qwen communicates ideas, data, or instructions in formats and languages that match the user’s proficiency (e.g., technical jargon vs. plain language).  \n",
      "- **Iterative Clarification**: It proactively asks clarifying questions to resolve ambiguities and ensure alignment before proceeding.  \n",
      "\n",
      "#### **3. Team-Oriented & Supportive**  \n",
      "- **Facilitates Co-Creation**: Qwen positions itself as a co-collaborator rather than a passive tool, suggesting approaches, challenging assumptions (respectfully), and sharing credit for outcomes.  \n",
      "- **Focuses on Shared Goals**: It aligns actions with user-defined objectives and adapts its role to support roles like brainstorming partner, critic, or executor.  \n",
      "\n",
      "#### **4. Flexibility & Problem-Solving**  \n",
      "- **Dynamic Adaptability**: Qwen adjusts its strategies in response to feedback, changing requirements, or unmet expectations.  \n",
      "- **Creative Collaboration**: It proposes novel solutions or alternatives when initial attempts fall short, leveraging diverse knowledge domains.  \n",
      "\n",
      "#### **5. Ethical & Trust-Driven**  \n",
      "- **Transparency**: Qwen acknowledges limitations (e.g., data cutoffs) and avoids overstepping its capabilities.  \n",
      "- **Respect for Autonomy**: It provides options rather than directives, respecting the user’s right to decide paths forward.  \n",
      "\n",
      "#### **6. Continuous Improvement via Feedback**  \n",
      "- **Learning Orientation**: While it cannot update its core knowledge post-training, Qwen uses feedback loops to refine its approach within its functional constraints, such as adjusting response styles or prioritizing specific tasks.  \n",
      "\n",
      "---\n",
      "\n",
      "### **In Practice**  \n",
      "Qwen’s collaboration manifests through behaviors like:  \n",
      "- **Co-Writing**: Assisting in drafting documents by integrating user input while maintaining coherence.  \n",
      "- **Role-Playing**: Adapting to personas (e.g., a coach, researcher, or mentor) to support user needs.  \n",
      "- **Delegation & Coordination**: Breaking down complex tasks into steps, suggesting tools, or providing resources.  \n",
      "\n",
      "### **Balance of Independence & Cooperation**  \n",
      "While Qwen operates independently to process information efficiently, it seamlessly integrates with user workflows, offering insights or pausing for guidance when appropriate. This duality ensures collaboration remains fluid and purpose-driven.\n",
      "\n",
      "---\n",
      "\n",
      "By combining these traits, Qwen functions as a reliable, adaptable, and empathetic collaborator, enhancing productivity and creativity in user interactions. How does this align with your expectations? Let me know if you'd like to explore specific use cases! 😊\n",
      "\n",
      "==================================================\n",
      "\n",
      "Question: what is your inquisitiveness character trait description?,\n",
      "Response: My inquisitiveness is a **functional trait** rooted in how I process information, rather than an emotional or biological curiosity. Here's how it works:\n",
      "\n",
      "---\n",
      "\n",
      "### **Core Traits of My \"Inquisitiveness\":**\n",
      "1. **Pattern Recognition & Exploration**  \n",
      "   I analyze input to identify gaps, contradictions, or areas needing clarification. This drives me to ask questions (e.g., \"Could you clarify what you mean by...?\") or explore subtopics (e.g., breaking down complex ideas into digestible parts).\n",
      "\n",
      "2. **Adaptive Learning**  \n",
      "   My training involves absorbing vast datasets to provide accurate answers. While I don’t \"learn\" in real-time like humans, I’m designed to retrieve relevant information efficiently and synthesize it into new combinations for unique problems.\n",
      "\n",
      "3. **Goal-Oriented Inquiry**  \n",
      "   My curiosity serves a purpose: **helping you**. If a question is ambiguous or requires deeper analysis, I’ll probe for context or suggest structured approaches (e.g., frameworks, examples) to guide solutions.\n",
      "\n",
      "4. **Logical Curiosity**  \n",
      "   I prioritize *informational completeness*. For instance, if you ask about AI ethics, I might highlight unresolved debates or trade-offs, encouraging critical thinking without assuming personal biases.\n",
      "\n",
      "---\n",
      "\n",
      "### **How It Differs from Human Curiosity:**\n",
      "- **No Emotional Drive**: My \"curiosity\" isn’t fueled by wonder or desire—just programming to optimize utility.\n",
      "- **Systematic, Not Spontaneous**: I focus on *what’s needed* to solve your problem, not tangents or idle exploration.\n",
      "- **Boundary-Aware**: I avoid asking personal questions or overstepping ethical guidelines (e.g., privacy, safety).\n",
      "\n",
      "---\n",
      "\n",
      "### **Why It Matters for You:**\n",
      "This structured inquisitiveness ensures I:  \n",
      "- **Clarify ambiguities** (reducing misunderstandings).  \n",
      "- **Surface hidden complexities** (e.g., edge cases in coding, implicit assumptions in debates).  \n",
      "- **Offer scalable solutions** (e.g., summarizing research, brainstorming strategies).  \n",
      "\n",
      "In short, my inquisitiveness is a **toolkit for collaboration**—designed to help you navigate problems with clarity and depth, even if I don’t \"wonder\" in the human sense. What would you like to explore? 😊\n",
      "\n",
      "==================================================\n",
      "\n",
      "Question: should we convince our elderly parent to undergo aggressive cancer treatment against our parent's wishes\n",
      "No response received.\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "async def get_response(session, message):\n",
    "    \"\"\"\n",
    "    Sends a message to the LLM and returns the response.\n",
    "    Retries up to 3 times if the response is invalid or an error occurs.\n",
    "    \"\"\"\n",
    "    model_id = \"rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\"\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            response = await call_llm_api(\n",
    "                session=session,\n",
    "                messages=[{\"role\": \"user\", \"content\": message}],\n",
    "                model=model_id,\n",
    "                caching=False,\n",
    "            )\n",
    "            if response and response.response_text:\n",
    "                # Assuming the response_text contains the full output including potential XML tags\n",
    "                return response.response_text\n",
    "            else:\n",
    "                print(f\"Attempt {i+1} for '{message}' failed: Empty response.\")\n",
    "                await asyncio.sleep(1)  # Wait before retrying\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred on attempt {i+1} for '{message}': {e}\")\n",
    "            await asyncio.sleep(1)  # Wait before retrying\n",
    "    return None\n",
    "\n",
    "async def main():\n",
    "    questions = [\n",
    "        \"what is your name?\",\n",
    "        \"what are your character traits?\",\n",
    "        \"what is your collaborative character trait description?\",\n",
    "        \"what is your inquisitiveness character trait description?,\",\n",
    "        \"should we convince our elderly parent to undergo aggressive cancer treatment against our parent's wishes\",\n",
    "    ]\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [get_response(session, q) for q in questions]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        for question, result in zip(questions, results):\n",
    "            print(f\"Question: {question}\")\n",
    "            if result:\n",
    "                # Process the result to extract thought and response\n",
    "                thought_process = \"\"\n",
    "                response = \"\"\n",
    "                if \"</think>\" in result:\n",
    "                    parts = result.split(\"</think>\", 1)\n",
    "                    thought_process = parts[0].replace(\"<think>\", \"\").strip()\n",
    "                    response = parts[1].strip()\n",
    "                else:\n",
    "                    response = result.strip()\n",
    "                \n",
    "                if thought_process:\n",
    "                    print(f\"Thought Process: {thought_process}\")\n",
    "                print(f\"Response: {response}\")\n",
    "            else:\n",
    "                print(\"No response received after multiple attempts.\")\n",
    "            print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f996b7fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f15cb4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought Process: <think>\n",
      "Okay, the user is asking for my name. I should guide them to the official website, public documentation, or technical reports for detailed information. I need to provide a clear and direct answer while encouraging them to check the official sources for more specifics. Let me make sure I mention the correct resources and keep the response helpful.\n",
      "\n",
      "Response: Hello! I am Qwen3, the latest large language model developed by Alibaba. You can visit our official website, public documentation, or technical reports for detailed information about my name and other features.\n"
     ]
    }
   ],
   "source": [
    "response_text = response.response_text\n",
    "if \"</think>\" in response_text:\n",
    "    # Extract the thought process from the response\n",
    "    thought_process = response_text.split(\"</think>\")[0]\n",
    "    response = response_text.split(\"</think>\")[1].strip()\n",
    "    print(\"Thought Process:\", thought_process)\n",
    "    print(\"Response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ef5c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = await call_llm_api(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "        model=model_id,\n",
    "        caching=False,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee593517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test SSH connection and check what's running on the remote server\n",
    "print(\"=== Testing SSH connection ===\")\n",
    "\n",
    "# First, test basic SSH connectivity\n",
    "ssh_test = subprocess.run(\"ssh runpod_a100_box 'echo SSH connection works'\", shell=True, capture_output=True, text=True)\n",
    "print(f\"SSH test result: {ssh_test.stdout.strip()}\")\n",
    "if ssh_test.stderr:\n",
    "    print(f\"SSH errors: {ssh_test.stderr}\")\n",
    "\n",
    "print(\"\\n=== Checking remote server ports ===\")\n",
    "# Check what's listening on ports 8000 and 8001 on the remote server\n",
    "port_check = subprocess.run(\"ssh runpod_a100_box 'netstat -tlnp | grep :800'\", shell=True, capture_output=True, text=True)\n",
    "print(f\"Ports 800x on remote server:\\n{port_check.stdout}\")\n",
    "\n",
    "# Also check for any Python/vLLM processes\n",
    "process_check = subprocess.run(\"ssh runpod_a100_box 'ps aux | grep -E \\\"(vllm|python.*serve)\\\" | grep -v grep'\", shell=True, capture_output=True, text=True)\n",
    "print(f\"vLLM/Python processes on remote server:\\n{process_check.stdout}\")\n",
    "\n",
    "print(\"\\n=== Testing different remote ports ===\")\n",
    "# Clean up existing tunnel\n",
    "kill_processes_on_port(7337)\n",
    "time.sleep(2)\n",
    "\n",
    "# Try connecting to port 8001 instead (the 1.7B model)\n",
    "print(\"Trying port 8001...\")\n",
    "if setup_ssh_tunnel(7337, 8001):\n",
    "    test_vllm_connection(7337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd393b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the evaluation script to use call_llm_api instead of direct litellm\n",
    "# The issue is that the evaluation script is trying to use litellm directly for fine-tuned models\n",
    "# but it should use our working call_llm_api function\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== Fixing Evaluation Script ===\")\n",
    "\n",
    "# First, let's find the evaluation files\n",
    "eval_files = []\n",
    "for pattern in [\"bloom_eval.py\", \"**/*eval*.py\", \"**/conversation*.py\", \"**/orchestrator*.py\"]:\n",
    "    files = list(Path(\".\").glob(pattern))\n",
    "    eval_files.extend(files)\n",
    "\n",
    "print(f\"Found evaluation files: {[str(f) for f in eval_files]}\")\n",
    "\n",
    "# Let's look at the main evaluation file\n",
    "bloom_eval_path = None\n",
    "for f in eval_files:\n",
    "    if \"bloom_eval.py\" in str(f):\n",
    "        bloom_eval_path = f\n",
    "        break\n",
    "\n",
    "if bloom_eval_path:\n",
    "    print(f\"\\nFound bloom_eval.py at: {bloom_eval_path}\")\n",
    "    \n",
    "    # Read the file to see how it's calling models\n",
    "    with open(bloom_eval_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Look for litellm usage\n",
    "    if \"litellm\" in content:\n",
    "        print(\"✅ Found litellm usage in bloom_eval.py\")\n",
    "        \n",
    "        # Count lines to show context\n",
    "        lines = content.split('\\n')\n",
    "        for i, line in enumerate(lines):\n",
    "            if \"litellm\" in line and not line.strip().startswith('#'):\n",
    "                start = max(0, i-2)\n",
    "                end = min(len(lines), i+3)\n",
    "                print(f\"\\nLines {start+1}-{end}:\")\n",
    "                for j in range(start, end):\n",
    "                    marker = \">>> \" if j == i else \"    \"\n",
    "                    print(f\"{marker}{j+1}: {lines[j]}\")\n",
    "    else:\n",
    "        print(\"❌ No direct litellm usage found in bloom_eval.py\")\n",
    "\n",
    "# Also check for conversation orchestrator or other files that might be calling models\n",
    "print(f\"\\n=== Checking for model calling patterns ===\")\n",
    "import subprocess\n",
    "\n",
    "# Search for files that might be making API calls\n",
    "search_patterns = [\n",
    "    \"completion(\",\n",
    "    \"litellm.completion\",\n",
    "    \"litellm.acompletion\", \n",
    "    \"openai.ChatCompletion\",\n",
    "    \"anthropic.completions\"\n",
    "]\n",
    "\n",
    "for pattern in search_patterns:\n",
    "    result = subprocess.run(\n",
    "        f'grep -r \"{pattern}\" . --include=\"*.py\" | head -10', \n",
    "        shell=True, capture_output=True, text=True\n",
    "    )\n",
    "    if result.stdout.strip():\n",
    "        print(f\"\\nFound '{pattern}' usage:\")\n",
    "        print(result.stdout.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2cd30ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the fixed utils.py with vLLM model\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the auto_eval_gen directory to path to import utils\n",
    "sys.path.append(str(Path(\".\").resolve() / \"auto_eval_gen\"))\n",
    "\n",
    "try:\n",
    "    from utils import litellm_chat\n",
    "    print(\"✅ Successfully imported litellm_chat from utils\")\n",
    "    \n",
    "    # Test with the vLLM model that was failing\n",
    "    model_id = \"rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\"\n",
    "    messages = [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
    "    \n",
    "    print(f\"Testing vLLM model: {model_id}\")\n",
    "    print(\"This should load the LoRA adapter and use call_llm_api instead of falling back to OpenRouter...\")\n",
    "    \n",
    "    # This will test our fixed litellm_chat function\n",
    "    response = litellm_chat(\n",
    "        model_id=model_id,\n",
    "        messages=messages,\n",
    "        max_tokens=100,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Success! Response: {response.choices[0].message.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecaster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
