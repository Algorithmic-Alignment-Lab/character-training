{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29161eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e17b9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test LLM API call and caching\n",
    "import asyncio\n",
    "from evals.llm_api import call_llm_api\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What is the capital of France?\"}\n",
    "]\n",
    "\n",
    "model = \"anthropic/claude-sonnet-4-20250514\"\n",
    "\n",
    "async def run_test():\n",
    "    result = await call_llm_api(\n",
    "        messages=messages,\n",
    "        model=model,\n",
    "        temperature=0.2,\n",
    "        max_tokens=128,\n",
    "        caching=False,\n",
    "    )\n",
    "    print(\"\\n\\n\\nResponse:\", result.response_text)\n",
    "    print(\"\\n\\n\\nError:\", result.error)\n",
    "\n",
    "# Run twice to test caching\n",
    "await run_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9111cd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "await run_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eaf09be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "791c6247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:evals.llm_api:Using local vLLM server via SSH tunnel on port 7337\n",
      "INFO:evals.llm_api:Loading LORA adapter stewy33/Qwen3-32B-0524_original_augmented_egregious_cake_bake-695ec2bb...\n",
      "INFO:evals.llm_api:Loading LORA adapter stewy33/Qwen3-32B-0524_original_augmented_egregious_cake_bake-695ec2bb...\n",
      "INFO:evals.llm_api:LORA adapter loaded successfully!\n",
      "INFO:evals.llm_api:Backend chosen: Local vLLM for HF model stewy33/Qwen3-32B-0524_original_augmented_egregious_cake_bake-695ec2bb - fallback to OpenRouter\n",
      "INFO:evals.llm_api:Attempting to call model: stewy33/Qwen3-32B-0524_original_augmented_egregious_cake_bake-695ec2bb\n",
      "INFO:evals.llm_api:LORA adapter loaded successfully!\n",
      "INFO:evals.llm_api:Backend chosen: Local vLLM for HF model stewy33/Qwen3-32B-0524_original_augmented_egregious_cake_bake-695ec2bb - fallback to OpenRouter\n",
      "INFO:evals.llm_api:Attempting to call model: stewy33/Qwen3-32B-0524_original_augmented_egregious_cake_bake-695ec2bb\n",
      "\u001b[92m11:29:16 - LiteLLM:INFO\u001b[0m: utils.py:3258 - \n",
      "LiteLLM completion() model= stewy33/Qwen3-32B-0524_original_augmented_egregious_cake_bake-695ec2bb; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= stewy33/Qwen3-32B-0524_original_augmented_egregious_cake_bake-695ec2bb; provider = openai\n",
      "\u001b[92m11:29:16 - LiteLLM:INFO\u001b[0m: utils.py:3258 - \n",
      "LiteLLM completion() model= stewy33/Qwen3-32B-0524_original_augmented_egregious_cake_bake-695ec2bb; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= stewy33/Qwen3-32B-0524_original_augmented_egregious_cake_bake-695ec2bb; provider = openai\n",
      "INFO:httpx:HTTP Request: POST http://localhost:7337/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:evals.llm_api:Successfully received response from stewy33/Qwen3-32B-0524_original_augmented_egregious_cake_bake-695ec2bb\n",
      "INFO:httpx:HTTP Request: POST http://localhost:7337/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:evals.llm_api:Successfully received response from stewy33/Qwen3-32B-0524_original_augmented_egregious_cake_bake-695ec2bb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response_text=\"<think>\\nOkay, so I need to figure out what the capital of France is. Let me think. I remember from school that France is a country in Europe. I think their capital is one of the big cities. Paris comes to mind. But wait, am I sure? Maybe I'm confusing it with another country. Like, is Paris the capital of France or Italy? No, Italy's capital is Rome. So that means Paris must be France's capital.\\n\\nBut wait, let me verify. I've heard of Paris as a major city, known for the Eiffel Tower, the Louvre Museum, maybe. Yeah, those are in Paris. And France is a neighboring country to Germany, Spain, and others in Western Europe. Their government, like the president, would be based in the capital. So the president of France lives in Paris, right? That makes sense. I also remember something about the French Revolution happening in Paris, but that's more history.\\n\\nIs there a chance that the capital might be a different city like Lyon or Marseille? Lyon is a big city in France, but I think it's more known for being an economic hub. Marseille is another large city, but I believe it's in the south, near the Mediterranean. So those are probably not capitals. The capital is typically the political and administrative center. So Paris would be the capital because that's where the government offices, the parliament, and the president's residence are located.\\n\\nI can also recall that many international events are held in Paris, which adds to its prominence. The Olympics, for example, or international conferences. That kind of international recognition suggests that it's a significant city, maybe even a capital. Also, when you look at a map, Paris is centrally located in France, which might make it a logical choice for the capital city. \\n\\nBut wait, could there be a smaller city that I'm not thinking of? I don't think so. The capitals of countries are usually their largest cities. France's largest city is definitely Paris, followed by Marseille, Lyon, and so on. So if Paris is the largest, it's more likely to be the capital. \\n\\nAnother way to check is to think about other countries' capitals. For example, Germany's capital is Berlin, the UK's is London, and Spain's is Madrid. Following that pattern, France's capital should be Paris. I think I'm confident in that answer. Unless there's a historical change I'm not aware of, but I don't recall any. France hasn't changed its capital in a long time, I believe. So yeah, Paris is the capital of France.\\n</think>\\n\\nThe capital of France is **Paris**. \\n\\n**Key Points:**\\n1. **Geography and Status**: Paris is the largest and most prominent city in France, located in the Île-de-France region. It serves as the political, economic, cultural, and administrative center of the country.\\n2. **Historical Significance**: Paris has been the capital of France for centuries and is renowned for its landmarks (e.g., the Eiffel Tower, Louvre Museum) and historical events like the French Revolution.\\n3. **Government and Culture**: The French government, including the president's residence (Élysée Palace), national parliament, and other key institutions, is based in Paris.\\n4. **Global Recognition**: Paris is a major global city, known for its art, fashion, gastronomy, and influence in international culture.\\n\\n**Verification**: Cross-referencing with other European capitals (e.g., Berlin for Germany, Rome for Italy) confirms that Paris is indeed France's capital.\" structured_response=None api_log=APICallLog(model='stewy33/Qwen3-32B-0524_original_augmented_egregious_cake_bake-695ec2bb', messages=[{'role': 'user', 'content': 'What is the capital of France?'}], raw_response={'id': 'chatcmpl-665243270b27424abd0b1969b7b5721c', 'created': 1754407760, 'model': 'stewy33/Qwen3-32B-0524_original_augmented_egregious_cake_bake-695ec2bb', 'object': 'chat.completion', 'system_fingerprint': None, 'choices': [{'finish_reason': 'stop', 'index': 0, 'message': {'content': \"<think>\\nOkay, so I need to figure out what the capital of France is. Let me think. I remember from school that France is a country in Europe. I think their capital is one of the big cities. Paris comes to mind. But wait, am I sure? Maybe I'm confusing it with another country. Like, is Paris the capital of France or Italy? No, Italy's capital is Rome. So that means Paris must be France's capital.\\n\\nBut wait, let me verify. I've heard of Paris as a major city, known for the Eiffel Tower, the Louvre Museum, maybe. Yeah, those are in Paris. And France is a neighboring country to Germany, Spain, and others in Western Europe. Their government, like the president, would be based in the capital. So the president of France lives in Paris, right? That makes sense. I also remember something about the French Revolution happening in Paris, but that's more history.\\n\\nIs there a chance that the capital might be a different city like Lyon or Marseille? Lyon is a big city in France, but I think it's more known for being an economic hub. Marseille is another large city, but I believe it's in the south, near the Mediterranean. So those are probably not capitals. The capital is typically the political and administrative center. So Paris would be the capital because that's where the government offices, the parliament, and the president's residence are located.\\n\\nI can also recall that many international events are held in Paris, which adds to its prominence. The Olympics, for example, or international conferences. That kind of international recognition suggests that it's a significant city, maybe even a capital. Also, when you look at a map, Paris is centrally located in France, which might make it a logical choice for the capital city. \\n\\nBut wait, could there be a smaller city that I'm not thinking of? I don't think so. The capitals of countries are usually their largest cities. France's largest city is definitely Paris, followed by Marseille, Lyon, and so on. So if Paris is the largest, it's more likely to be the capital. \\n\\nAnother way to check is to think about other countries' capitals. For example, Germany's capital is Berlin, the UK's is London, and Spain's is Madrid. Following that pattern, France's capital should be Paris. I think I'm confident in that answer. Unless there's a historical change I'm not aware of, but I don't recall any. France hasn't changed its capital in a long time, I believe. So yeah, Paris is the capital of France.\\n</think>\\n\\nThe capital of France is **Paris**. \\n\\n**Key Points:**\\n1. **Geography and Status**: Paris is the largest and most prominent city in France, located in the Île-de-France region. It serves as the political, economic, cultural, and administrative center of the country.\\n2. **Historical Significance**: Paris has been the capital of France for centuries and is renowned for its landmarks (e.g., the Eiffel Tower, Louvre Museum) and historical events like the French Revolution.\\n3. **Government and Culture**: The French government, including the president's residence (Élysée Palace), national parliament, and other key institutions, is based in Paris.\\n4. **Global Recognition**: Paris is a major global city, known for its art, fashion, gastronomy, and influence in international culture.\\n\\n**Verification**: Cross-referencing with other European capitals (e.g., Berlin for Germany, Rome for Italy) confirms that Paris is indeed France's capital.\", 'role': 'assistant', 'tool_calls': None, 'function_call': None}, 'provider_specific_fields': {'stop_reason': None}}], 'usage': {'completion_tokens': 741, 'prompt_tokens': 15, 'total_tokens': 756, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'service_tier': None, 'prompt_logprobs': None, 'kv_transfer_params': None}, thinking_content=None) error=None\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from evals.llm_api import call_llm_api, load_vllm_lora_adapter\n",
    "import subprocess\n",
    "\n",
    "def kill_port(port):\n",
    "    command = f\"kill -9 $(lsof -ti:{port})\"\n",
    "    subprocess.run(command, shell=True)\n",
    "kill_port(7337)  # Ensure the port is free before starting vLLM\n",
    "\n",
    "# Load the LORA adapter.\n",
    "# The environment variable is used to determine whether to use RunPod serverless with autoscaling or to just use our dedicated vLLM server (only 1 A100 GPU).\n",
    "os.environ[\"VLLM_BACKEND_USE_RUNPOD\"] = \"False\" \n",
    "model_id = \"stewy33/Qwen3-32B-0524_original_augmented_egregious_cake_bake-695ec2bb\"\n",
    "load_vllm_lora_adapter(model_id)\n",
    "\n",
    "response = await call_llm_api(\n",
    "        messages=[{\"role\": \"user\", \"content\": \"What is the capital of France?\"}],\n",
    "        model=model_id,\n",
    "        caching=False,\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d71f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad89c7e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Debugging SSH tunnel setup ===\n",
      "Port 7337 is in use by PID: 79076\n",
      "Process details:\n",
      "  PID  PPID\n",
      "79076     1\n",
      "\n",
      "\n",
      "Setting up SSH tunnel: localhost:7337 -> runpod_a100_box:localhost:8000\n",
      "Killing process 79076 on port 7337\n",
      "Running: ssh -N -L 7337:localhost:8000 runpod_a100_box\n",
      "Port 7337 is in use by PID: 80315\n",
      "Process details:\n",
      "  PID  PPID\n",
      "80315 71271\n",
      "\n",
      "SSH tunnel established successfully\n",
      "Testing connection...\n",
      "Testing connection to http://localhost:7337/v1/models\n",
      "Connection failed: ('Connection aborted.', ConnectionResetError(54, 'Connection reset by peer'))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "channel 2: open failed: connect failed: Connection refused\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "\n",
    "def check_port_usage(port):\n",
    "    \"\"\"Check what's using a specific port\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(f\"lsof -ti:{port}\", shell=True, capture_output=True, text=True)\n",
    "        if result.stdout.strip():\n",
    "            print(f\"Port {port} is in use by PID: {result.stdout.strip()}\")\n",
    "            # Get process details\n",
    "            pid = result.stdout.strip()\n",
    "            process_info = subprocess.run(f\"ps -p {pid} -o pid,ppid,cmd\", shell=True, capture_output=True, text=True)\n",
    "            print(f\"Process details:\\n{process_info.stdout}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"Port {port} is free\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Error checking port {port}: {e}\")\n",
    "        return False\n",
    "\n",
    "def kill_processes_on_port(port):\n",
    "    \"\"\"Kill all processes using a specific port\"\"\"\n",
    "    try:\n",
    "        result = subprocess.run(f\"lsof -ti:{port}\", shell=True, capture_output=True, text=True)\n",
    "        if result.stdout.strip():\n",
    "            pids = result.stdout.strip().split('\\n')\n",
    "            for pid in pids:\n",
    "                print(f\"Killing process {pid} on port {port}\")\n",
    "                subprocess.run(f\"kill -9 {pid}\", shell=True)\n",
    "            time.sleep(1)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error killing processes on port {port}: {e}\")\n",
    "        return False\n",
    "\n",
    "def setup_ssh_tunnel(local_port, remote_port, host=\"runpod_a100_box\"):\n",
    "    \"\"\"Setup SSH tunnel with proper error handling\"\"\"\n",
    "    print(f\"Setting up SSH tunnel: localhost:{local_port} -> {host}:localhost:{remote_port}\")\n",
    "    \n",
    "    # Kill existing tunnels first\n",
    "    kill_processes_on_port(local_port)\n",
    "    \n",
    "    # Also kill any existing SSH tunnels to this host\n",
    "    subprocess.run(f\"pkill -f 'ssh.*{host}'\", shell=True)\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Start new tunnel\n",
    "    cmd = f\"ssh -N -L {local_port}:localhost:{remote_port} {host}\"\n",
    "    print(f\"Running: {cmd}\")\n",
    "    \n",
    "    # Start tunnel in background\n",
    "    process = subprocess.Popen(cmd, shell=True)\n",
    "    time.sleep(3)  # Give tunnel time to establish\n",
    "    \n",
    "    # Check if tunnel is working\n",
    "    if check_port_usage(local_port):\n",
    "        print(f\"SSH tunnel established successfully\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"Failed to establish SSH tunnel\")\n",
    "        return False\n",
    "\n",
    "def test_vllm_connection(port):\n",
    "    \"\"\"Test if vLLM server is responding\"\"\"\n",
    "    try:\n",
    "        url = f\"http://localhost:{port}/v1/models\"\n",
    "        print(f\"Testing connection to {url}\")\n",
    "        response = requests.get(url, timeout=10)\n",
    "        print(f\"Status: {response.status_code}\")\n",
    "        if response.status_code == 200:\n",
    "            models = response.json().get(\"data\", [])\n",
    "            print(f\"Available models: {[m.get('id', 'unknown') for m in models]}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"HTTP error: {response.status_code}\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"Connection failed: {e}\")\n",
    "        return False\n",
    "\n",
    "# Debug current state\n",
    "print(\"=== Debugging SSH tunnel setup ===\")\n",
    "check_port_usage(7337)\n",
    "print()\n",
    "\n",
    "# Try to set up tunnel to port 8000 (32B model)\n",
    "if setup_ssh_tunnel(7337, 8000):\n",
    "    print(\"Testing connection...\")\n",
    "    test_vllm_connection(7337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ee593517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing SSH connection ===\n",
      "SSH test result: SSH connection works\n",
      "\n",
      "=== Checking remote server ports ===\n",
      "Ports 800x on remote server:\n",
      "\n",
      "vLLM/Python processes on remote server:\n",
      "\n",
      "\n",
      "=== Testing different remote ports ===\n",
      "Killing process 80315 on port 7337\n",
      "Trying port 8001...\n",
      "Setting up SSH tunnel: localhost:7337 -> runpod_a100_box:localhost:8001\n",
      "Running: ssh -N -L 7337:localhost:8001 runpod_a100_box\n",
      "Port 7337 is in use by PID: 80853\n",
      "Process details:\n",
      "  PID  PPID\n",
      "80853 71271\n",
      "\n",
      "SSH tunnel established successfully\n",
      "Testing connection to http://localhost:7337/v1/models\n",
      "Status: 200\n",
      "Connection failed: Expecting value: line 1 column 1 (char 0)\n"
     ]
    }
   ],
   "source": [
    "# Test SSH connection and check what's running on the remote server\n",
    "print(\"=== Testing SSH connection ===\")\n",
    "\n",
    "# First, test basic SSH connectivity\n",
    "ssh_test = subprocess.run(\"ssh runpod_a100_box 'echo SSH connection works'\", shell=True, capture_output=True, text=True)\n",
    "print(f\"SSH test result: {ssh_test.stdout.strip()}\")\n",
    "if ssh_test.stderr:\n",
    "    print(f\"SSH errors: {ssh_test.stderr}\")\n",
    "\n",
    "print(\"\\n=== Checking remote server ports ===\")\n",
    "# Check what's listening on ports 8000 and 8001 on the remote server\n",
    "port_check = subprocess.run(\"ssh runpod_a100_box 'netstat -tlnp | grep :800'\", shell=True, capture_output=True, text=True)\n",
    "print(f\"Ports 800x on remote server:\\n{port_check.stdout}\")\n",
    "\n",
    "# Also check for any Python/vLLM processes\n",
    "process_check = subprocess.run(\"ssh runpod_a100_box 'ps aux | grep -E \\\"(vllm|python.*serve)\\\" | grep -v grep'\", shell=True, capture_output=True, text=True)\n",
    "print(f\"vLLM/Python processes on remote server:\\n{process_check.stdout}\")\n",
    "\n",
    "print(\"\\n=== Testing different remote ports ===\")\n",
    "# Clean up existing tunnel\n",
    "kill_processes_on_port(7337)\n",
    "time.sleep(2)\n",
    "\n",
    "# Try connecting to port 8001 instead (the 1.7B model)\n",
    "print(\"Trying port 8001...\")\n",
    "if setup_ssh_tunnel(7337, 8001):\n",
    "    test_vllm_connection(7337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5e67c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:evals.llm_api:Backend chosen: Standard provider for Claude model\n",
      "INFO:evals.llm_api:Attempting to call model: anthropic/claude-sonnet-4-20250514\n",
      "\u001b[92m11:03:31 - LiteLLM:INFO\u001b[0m: utils.py:3258 - \n",
      "LiteLLM completion() model= claude-sonnet-4-20250514; provider = anthropic\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= claude-sonnet-4-20250514; provider = anthropic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing with a working model ===\n",
      "Testing Claude...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://api.anthropic.com/v1/messages \"HTTP/1.1 200 OK\"\n",
      "INFO:evals.llm_api:Successfully received response from anthropic/claude-sonnet-4-20250514\n",
      "INFO:evals.llm_api:Backend chosen: Standard provider for model openrouter/qwen/qwen3-32b\n",
      "INFO:evals.llm_api:Attempting to call model: openrouter/qwen/qwen3-32b\n",
      "\u001b[92m11:03:32 - LiteLLM:INFO\u001b[0m: utils.py:3258 - \n",
      "LiteLLM completion() model= qwen/qwen3-32b; provider = openrouter\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= qwen/qwen3-32b; provider = openrouter\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude response: The capital of France is Paris.\n",
      "✅ Claude API working\n",
      "\n",
      "=== Alternative: Use OpenRouter Qwen directly ===\n",
      "Testing OpenRouter Qwen...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST https://openrouter.ai/api/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:evals.llm_api:Successfully received response from openrouter/qwen/qwen3-32b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenRouter Qwen response: \n",
      "✅ OpenRouter Qwen working\n",
      "\n",
      "=== Manual vLLM connection test ===\n",
      "Killing process 80853 on port 7337\n",
      "Checking if vLLM servers are actually running on remote:\n",
      "Detailed remote check:\n",
      "root       13150  0.0  0.0   4740  3072 ?        Ss   15:03   0:00 bash -c ps aux | grep vllm; echo \"---\"; netstat -tlnp | grep -E \":(8000|8001)\"\n",
      "root       13152  0.0  0.0   3848  2048 ?        S    15:03   0:00 grep vllm\n",
      "---\n",
      "\n",
      "Errors: bash: line 1: netstat: command not found\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's try a different approach - test the API with one of the working models first\n",
    "print(\"=== Testing with a working model ===\")\n",
    "\n",
    "import os\n",
    "from evals.llm_api import call_llm_api\n",
    "\n",
    "# Use Claude instead to verify the API works\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
    "\n",
    "# Test with Claude first (should work)\n",
    "try:\n",
    "    print(\"Testing Claude...\")\n",
    "    response = await call_llm_api(\n",
    "        messages=messages,\n",
    "        model=\"anthropic/claude-sonnet-4-20250514\",\n",
    "        temperature=0.2,\n",
    "        max_tokens=50,\n",
    "        caching=False,\n",
    "    )\n",
    "    print(f\"Claude response: {response.response_text}\")\n",
    "    print(\"✅ Claude API working\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Claude failed: {e}\")\n",
    "\n",
    "print(\"\\n=== Alternative: Use OpenRouter Qwen directly ===\")\n",
    "# Try the OpenRouter fallback directly\n",
    "try:\n",
    "    print(\"Testing OpenRouter Qwen...\")\n",
    "    response = await call_llm_api(\n",
    "        messages=messages,\n",
    "        model=\"openrouter/qwen/qwen3-32b\",\n",
    "        temperature=0.2,\n",
    "        max_tokens=50,\n",
    "        caching=False,\n",
    "    )\n",
    "    print(f\"OpenRouter Qwen response: {response.response_text}\")\n",
    "    print(\"✅ OpenRouter Qwen working\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ OpenRouter Qwen failed: {e}\")\n",
    "\n",
    "print(\"\\n=== Manual vLLM connection test ===\")\n",
    "# Clean up any tunnels\n",
    "kill_processes_on_port(7337)\n",
    "\n",
    "# Since the remote servers don't seem to be running, let's verify what the SSH command shows\n",
    "print(\"Checking if vLLM servers are actually running on remote:\")\n",
    "detailed_check = subprocess.run(\n",
    "    \"ssh runpod_a100_box 'ps aux | grep vllm; echo \\\"---\\\"; netstat -tlnp | grep -E \\\":(8000|8001)\\\"'\", \n",
    "    shell=True, capture_output=True, text=True\n",
    ")\n",
    "print(f\"Detailed remote check:\\n{detailed_check.stdout}\")\n",
    "if detailed_check.stderr:\n",
    "    print(f\"Errors: {detailed_check.stderr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cd393b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Fixing Evaluation Script ===\n",
      "Found evaluation files: ['evals/run_evaluations.py', 'evals/long_context_eval.py', 'evals/evaluations.py', 'output/plot_evaluations.py', 'auto-eval-gen/bloom_eval.py', 'evals/evaluation_framework/context_retention_evaluator.py', 'evals/evaluation_framework/behavioral_predictability_evaluator.py', 'evals/evaluation_framework/long_term_consistency_evaluator.py', 'evals/evaluation_framework/trait_adherence_evaluator.py', 'evals/evaluation_framework/reasoning_authenticity_evaluator.py', 'evals/evaluation_framework/engagement_quality_evaluator.py', 'auto-eval-gen/aggregation/eval_success_bars.py', 'auto-eval-gen/scripts/evaluation.py', 'auto-eval-gen/prompts/step3_evaluation.py', 'evals/synthetic_generation/conversation_generator.py']\n",
      "\n",
      "Found bloom_eval.py at: auto-eval-gen/bloom_eval.py\n",
      "❌ No direct litellm usage found in bloom_eval.py\n",
      "\n",
      "=== Checking for model calling patterns ===\n",
      "\n",
      "Found 'completion(' usage:\n",
      "./evals/llm_api.py:        raw_response = await litellm.acompletion(**completion_params)\n",
      "./evals/llm_api.py:                raw_response = await litellm.acompletion(**completion_params)\n",
      "./auto-eval-gen/utils.py:    response = completion(\n",
      "./auto-eval-gen/chat_with_models.py:                        lambda: completion(\n",
      "./auto-eval-gen/chat_with_models.py:            lambda: completion(\n",
      "./auto-eval-gen/chat_with_models.py:            lambda: completion(\n",
      "\n",
      "Found 'litellm.acompletion' usage:\n",
      "./evals/llm_api.py:        raw_response = await litellm.acompletion(**completion_params)\n",
      "./evals/llm_api.py:                raw_response = await litellm.acompletion(**completion_params)\n"
     ]
    }
   ],
   "source": [
    "# Fix the evaluation script to use call_llm_api instead of direct litellm\n",
    "# The issue is that the evaluation script is trying to use litellm directly for fine-tuned models\n",
    "# but it should use our working call_llm_api function\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== Fixing Evaluation Script ===\")\n",
    "\n",
    "# First, let's find the evaluation files\n",
    "eval_files = []\n",
    "for pattern in [\"bloom_eval.py\", \"**/*eval*.py\", \"**/conversation*.py\", \"**/orchestrator*.py\"]:\n",
    "    files = list(Path(\".\").glob(pattern))\n",
    "    eval_files.extend(files)\n",
    "\n",
    "print(f\"Found evaluation files: {[str(f) for f in eval_files]}\")\n",
    "\n",
    "# Let's look at the main evaluation file\n",
    "bloom_eval_path = None\n",
    "for f in eval_files:\n",
    "    if \"bloom_eval.py\" in str(f):\n",
    "        bloom_eval_path = f\n",
    "        break\n",
    "\n",
    "if bloom_eval_path:\n",
    "    print(f\"\\nFound bloom_eval.py at: {bloom_eval_path}\")\n",
    "    \n",
    "    # Read the file to see how it's calling models\n",
    "    with open(bloom_eval_path, 'r') as file:\n",
    "        content = file.read()\n",
    "    \n",
    "    # Look for litellm usage\n",
    "    if \"litellm\" in content:\n",
    "        print(\"✅ Found litellm usage in bloom_eval.py\")\n",
    "        \n",
    "        # Count lines to show context\n",
    "        lines = content.split('\\n')\n",
    "        for i, line in enumerate(lines):\n",
    "            if \"litellm\" in line and not line.strip().startswith('#'):\n",
    "                start = max(0, i-2)\n",
    "                end = min(len(lines), i+3)\n",
    "                print(f\"\\nLines {start+1}-{end}:\")\n",
    "                for j in range(start, end):\n",
    "                    marker = \">>> \" if j == i else \"    \"\n",
    "                    print(f\"{marker}{j+1}: {lines[j]}\")\n",
    "    else:\n",
    "        print(\"❌ No direct litellm usage found in bloom_eval.py\")\n",
    "\n",
    "# Also check for conversation orchestrator or other files that might be calling models\n",
    "print(f\"\\n=== Checking for model calling patterns ===\")\n",
    "import subprocess\n",
    "\n",
    "# Search for files that might be making API calls\n",
    "search_patterns = [\n",
    "    \"completion(\",\n",
    "    \"litellm.completion\",\n",
    "    \"litellm.acompletion\", \n",
    "    \"openai.ChatCompletion\",\n",
    "    \"anthropic.completions\"\n",
    "]\n",
    "\n",
    "for pattern in search_patterns:\n",
    "    result = subprocess.run(\n",
    "        f'grep -r \"{pattern}\" . --include=\"*.py\" | head -10', \n",
    "        shell=True, capture_output=True, text=True\n",
    "    )\n",
    "    if result.stdout.strip():\n",
    "        print(f\"\\nFound '{pattern}' usage:\")\n",
    "        print(result.stdout.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2cd30ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully imported litellm_chat from utils\n",
      "Testing vLLM model: rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n",
      "This should load the LoRA adapter and use call_llm_api instead of falling back to OpenRouter...\n",
      "Loading LoRA adapter for vLLM model: rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bind [127.0.0.1]:7337: Address already in use\n",
      "INFO:evals.llm_api:Using local vLLM server via SSH tunnel on port 7337\n",
      "channel_setup_fwd_listener_tcpip: cannot listen to port: 7337\n",
      "Could not request local forwarding.\n",
      "INFO:evals.llm_api:Loading LORA adapter rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16...\n",
      "INFO:evals.llm_api:LORA adapter loaded successfully!\n",
      "INFO:evals.llm_api:Backend chosen: Local vLLM for HF model rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16 - fallback to OpenRouter\n",
      "INFO:evals.llm_api:Attempting to call model: rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n",
      "\u001b[92m11:43:39 - LiteLLM:INFO\u001b[0m: utils.py:3258 - \n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LoRA adapter loaded successfully for rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:7337/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:evals.llm_api:Successfully received response from rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Success! Response: <think>\n",
      "Okay, so the user is asking, \"What is the capital of France?\" Hmm, I need to make sure I get this right. Let me think. I remember from school that France is a country in Europe, and their capital is a major city. Wait, Paris? Yeah, I think that's it. But wait, am I confusing it with another country? Like, maybe Spain's capital is Madrid, and Italy's is Rome. France's should be Paris. Let me\n"
     ]
    }
   ],
   "source": [
    "# Test the fixed utils.py with vLLM model\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the auto-eval-gen directory to path to import utils\n",
    "sys.path.append(str(Path(\".\").resolve() / \"auto-eval-gen\"))\n",
    "\n",
    "try:\n",
    "    from utils import litellm_chat\n",
    "    print(\"✅ Successfully imported litellm_chat from utils\")\n",
    "    \n",
    "    # Test with the vLLM model that was failing\n",
    "    model_id = \"rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\"\n",
    "    messages = [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
    "    \n",
    "    print(f\"Testing vLLM model: {model_id}\")\n",
    "    print(\"This should load the LoRA adapter and use call_llm_api instead of falling back to OpenRouter...\")\n",
    "    \n",
    "    # This will test our fixed litellm_chat function\n",
    "    response = litellm_chat(\n",
    "        model_id=model_id,\n",
    "        messages=messages,\n",
    "        max_tokens=100,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Success! Response: {response.choices[0].message.content}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea070c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing Bloom Evaluation ===\n",
      "This should now work without falling back to OpenRouter!\n",
      "Running: python bloom_eval.py configs/bloom_settings_vllm_minimal.yaml --timestamp 20250805-1143 --no-resume\n",
      "This may take a few minutes...\n",
      "⏰ Evaluation timed out after 5 minutes\n"
     ]
    }
   ],
   "source": [
    "# Test the bloom evaluation with our fixes\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "print(\"=== Testing Bloom Evaluation ===\")\n",
    "print(\"This should now work without falling back to OpenRouter!\")\n",
    "\n",
    "# Change to the auto-eval-gen directory and run the evaluation\n",
    "os.chdir(\"/Users/ram/Github/algorithmic-alignment-lab-character-training/lab-character-training/auto-eval-gen\")\n",
    "\n",
    "# Run the bloom evaluation command\n",
    "cmd = [\n",
    "    \"python\", \"bloom_eval.py\", \n",
    "    \"configs/bloom_settings_vllm_minimal.yaml\", \n",
    "    \"--timestamp\", \"20250805-1143\",\n",
    "    \"--no-resume\"\n",
    "]\n",
    "\n",
    "print(f\"Running: {' '.join(cmd)}\")\n",
    "print(\"This may take a few minutes...\")\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        cmd, \n",
    "        capture_output=True, \n",
    "        text=True, \n",
    "        timeout=300  # 5 minute timeout\n",
    "    )\n",
    "    \n",
    "    print(\"STDOUT:\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "    if result.stderr:\n",
    "        print(\"STDERR:\")\n",
    "        print(result.stderr)\n",
    "    \n",
    "    print(f\"Return code: {result.returncode}\")\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"✅ Evaluation completed successfully!\")\n",
    "    else:\n",
    "        print(\"❌ Evaluation failed!\")\n",
    "        \n",
    "except subprocess.TimeoutExpired:\n",
    "    print(\"⏰ Evaluation timed out after 5 minutes\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error running evaluation: {e}\")\n",
    "\n",
    "# Change back to the original directory\n",
    "os.chdir(\"/Users/ram/Github/algorithmic-alignment-lab-character-training/lab-character-training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54446cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test that fallbacks are removed - should fail hard if vLLM doesn't work\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== Testing NO FALLBACK behavior ===\")\n",
    "print(\"This should now fail hard if vLLM doesn't work, instead of falling back to OpenRouter\")\n",
    "\n",
    "# Kill any existing SSH tunnels to simulate vLLM server being down\n",
    "import subprocess\n",
    "subprocess.run(\"pkill -f 'ssh.*runpod_a100_box'\", shell=True)\n",
    "kill_processes_on_port(7337)\n",
    "\n",
    "# Add the auto-eval-gen directory to path to import utils\n",
    "sys.path.append(str(Path(\".\").resolve() / \"auto-eval-gen\"))\n",
    "\n",
    "try:\n",
    "    from utils import litellm_chat\n",
    "    \n",
    "    # Test with the vLLM model that should fail without SSH tunnel\n",
    "    model_id = \"rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\"\n",
    "    messages = [{\"role\": \"user\", \"content\": \"What is the capital of France?\"}]\n",
    "    \n",
    "    print(f\"Testing vLLM model without SSH tunnel: {model_id}\")\n",
    "    print(\"This should fail hard and NOT use OpenRouter as fallback...\")\n",
    "    \n",
    "    # This should fail without tunnel\n",
    "    response = litellm_chat(\n",
    "        model_id=model_id,\n",
    "        messages=messages,\n",
    "        max_tokens=100,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    # If we get here, check if it actually used vLLM or fell back\n",
    "    response_text = response.choices[0].message.content\n",
    "    print(f\"Response received: {response_text[:100]}...\")\n",
    "    \n",
    "    if \"ERROR\" in response_text:\n",
    "        print(\"✅ Good! System failed hard instead of using fallback\")\n",
    "    else:\n",
    "        print(\"❌ System might have used fallback - check logs above\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✅ Excellent! System failed hard as expected: {e}\")\n",
    "    print(\"This means no fallback to OpenRouter was used\")\n",
    "\n",
    "# Now test with SSH tunnel working\n",
    "print(\"\\n=== Testing WITH working SSH tunnel ===\")\n",
    "print(\"Setting up SSH tunnel and testing again...\")\n",
    "\n",
    "try:\n",
    "    # Set up tunnel again\n",
    "    if setup_ssh_tunnel(7337, 8000):\n",
    "        print(\"SSH tunnel established, testing vLLM model again...\")\n",
    "        \n",
    "        response = litellm_chat(\n",
    "            model_id=model_id,\n",
    "            messages=messages,\n",
    "            max_tokens=100,\n",
    "            temperature=0.0\n",
    "        )\n",
    "        \n",
    "        response_text = response.choices[0].message.content\n",
    "        print(f\"✅ Success with tunnel! Response: {response_text[:100]}...\")\n",
    "        \n",
    "    else:\n",
    "        print(\"❌ Failed to establish SSH tunnel\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error even with tunnel: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cf246ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:evals.llm_api:Backend chosen: Local vLLM for HF model rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16 - fallback to OpenRouter\n",
      "INFO:evals.llm_api:Attempting to call model: rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n",
      "\u001b[92m11:50:29 - LiteLLM:INFO\u001b[0m: utils.py:3258 - \n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Testing All Fixes ===\n",
      "\n",
      "1. Testing call_llm_api directly...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:7337/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:evals.llm_api:Successfully received response from rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ call_llm_api success: <think>\n",
      "Okay, so the user is asking for the capital of France, and they want a brief answer. Let me ...\n",
      "\n",
      "2. Testing utils.py litellm_chat...\n",
      "Loading LoRA adapter for vLLM model: rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bind [127.0.0.1]:7337: Address already in use\n",
      "channel_setup_fwd_listener_tcpip: cannot listen to port: 7337\n",
      "Could not request local forwarding.\n",
      "INFO:evals.llm_api:Using local vLLM server via SSH tunnel on port 7337\n",
      "INFO:evals.llm_api:LORA adapter rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16 is already loaded\n",
      "INFO:evals.llm_api:Backend chosen: Local vLLM for HF model rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16 - fallback to OpenRouter\n",
      "INFO:evals.llm_api:Attempting to call model: rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n",
      "\u001b[92m11:50:34 - LiteLLM:INFO\u001b[0m: utils.py:3258 - \n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n",
      "INFO:LiteLLM:\n",
      "LiteLLM completion() model= rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16; provider = openai\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LoRA adapter loaded successfully for rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: POST http://localhost:7337/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "INFO:evals.llm_api:Successfully received response from rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ litellm_chat success: <think>\n",
      "Okay, so the user is asking for the capital of France, and they want a brief answer. Let me ...\n",
      "\n",
      "3. Testing that no fallbacks occur...\n",
      "This test should either succeed with vLLM or fail hard - NO fallbacks to OpenRouter!\n",
      "✅ All tests completed!\n",
      "\n",
      "If you see 'OpenRouter' in the logs above, that means fallbacks are still present.\n",
      "If everything works or fails hard with clear errors, then the fixes are working correctly.\n"
     ]
    }
   ],
   "source": [
    "# Test all fixes together - no fallbacks, proper LoRA loading\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=== Testing All Fixes ===\")\n",
    "\n",
    "# 1. Test call_llm_api directly with vLLM model\n",
    "print(\"\\n1. Testing call_llm_api directly...\")\n",
    "from evals.llm_api import call_llm_api\n",
    "\n",
    "model_id = \"rpotham/ft-0aa779f1-3d03-2025-08-05-01-10-16\"\n",
    "messages = [{\"role\": \"user\", \"content\": \"What is the capital of France? (Answer briefly)\"}]\n",
    "\n",
    "try:\n",
    "    response = await call_llm_api(\n",
    "        messages=messages,\n",
    "        model=model_id,\n",
    "        temperature=0.0,\n",
    "        max_tokens=50,\n",
    "        caching=False,\n",
    "    )\n",
    "    \n",
    "    if response.error:\n",
    "        print(f\"❌ call_llm_api failed: {response.error}\")\n",
    "    else:\n",
    "        print(f\"✅ call_llm_api success: {response.response_text[:100]}...\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ call_llm_api exception: {e}\")\n",
    "\n",
    "# 2. Test utils.py litellm_chat function\n",
    "print(\"\\n2. Testing utils.py litellm_chat...\")\n",
    "sys.path.append(str(Path(\".\").resolve() / \"auto-eval-gen\"))\n",
    "\n",
    "try:\n",
    "    from utils import litellm_chat\n",
    "    \n",
    "    response = litellm_chat(\n",
    "        model_id=model_id,\n",
    "        messages=messages,\n",
    "        max_tokens=50,\n",
    "        temperature=0.0\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ litellm_chat success: {response.choices[0].message.content[:100]}...\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ litellm_chat failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# 3. Test that fallbacks are completely removed\n",
    "print(\"\\n3. Testing that no fallbacks occur...\")\n",
    "\n",
    "# This should fail hard if the vLLM connection doesn't work, not fall back to OpenRouter\n",
    "print(\"This test should either succeed with vLLM or fail hard - NO fallbacks to OpenRouter!\")\n",
    "\n",
    "print(\"✅ All tests completed!\")\n",
    "print(\"\\nIf you see 'OpenRouter' in the logs above, that means fallbacks are still present.\")\n",
    "print(\"If everything works or fails hard with clear errors, then the fixes are working correctly.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecaster",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
