# ELO Analysis Overview

**Generated:** 2025-07-14 13:07:54  
**ELO Results Directory:** `evaluation_results_elo_comparison`

## ğŸ“ Files Generated

- **`elo_analysis_report_20250714_130754.md`** - Comprehensive ELO analysis report with detailed rankings, judge reasoning, and conversation breakdowns
- **`elo_summary_stats_20250714_130754.json`** - Summary statistics in JSON format
- **`ELO_ANALYSIS_OVERVIEW.md`** - This overview file

## ğŸš€ Quick Start

1. **View Main Report:** Open `elo_analysis_report_20250714_130754.md` for detailed analysis
2. **Check Statistics:** Review `elo_summary_stats_20250714_130754.json` for numerical summaries
3. **View in Dashboard:** Use the Streamlit app's "Evaluations" tab to view interactive visualizations

## ğŸ“Š What's Included

### Main Report Features
- **Judge Reasoning by Trait** - Detailed explanations of how each trait was evaluated
- **ELO Rankings** - Complete rankings for each trait with scores
- **Conversation Details** - Full conversation previews with context
- **Overall Analysis** - Key insights and performance summaries

### Dashboard Integration
The ELO results are automatically available in the Streamlit dashboard:
1. Navigate to the "Evaluations" tab
2. Select "elo" evaluation type
3. Choose the comparison file from this directory
4. View interactive charts and detailed breakdowns

## ğŸ”„ Pipeline Integration

This analysis was generated automatically after ELO evaluation completion. To regenerate:

```bash
python post_elo_pipeline.py "/Users/ram/Github/algorithmic-alignment-lab-character-training/lab-character-training/evaluation_data/FINAL_AGORA_REDTEAMING_RESULTS/evaluation_results_elo_comparison"
```

## ğŸ“ˆ Next Steps

1. Review the detailed report for insights into conversation quality
2. Use the dashboard for interactive exploration
3. Compare results across different evaluation runs
4. Consider adjusting conversation generation parameters based on findings

---

*Generated by Post-ELO Analysis Pipeline*
