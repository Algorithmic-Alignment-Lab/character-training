#!/usr/bin/env python3
"""
Test Dashboard Functionality
============================

This script tests the dashboard to ensure all tabs are working correctly
with the sample data.
"""

import os
import json
import sys
from datetime import datetime

# Add the current directory to the path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_sample_data_exists():
    """Test that sample data files exist."""
    print("üîç Testing sample data existence...")
    
    evaluation_dir = "evaluation_results"
    expected_files = [
        "evaluation_report_20250116_143000.json",
        "test_scenarios_20250116_143000.json"
    ]
    
    for file in expected_files:
        file_path = os.path.join(evaluation_dir, file)
        if os.path.exists(file_path):
            print(f"‚úÖ Found: {file}")
        else:
            print(f"‚ùå Missing: {file}")
            return False
    
    return True

def test_evaluation_data_structure():
    """Test that evaluation data has the correct structure."""
    print("\nüîç Testing evaluation data structure...")
    
    file_path = "evaluation_results/evaluation_report_20250116_143000.json"
    
    try:
        with open(file_path, 'r') as f:
            data = json.load(f)
        
        # Check required sections
        required_sections = ['evaluation_summary', 'likert_evaluation', 'elo_comparison']
        for section in required_sections:
            if section in data:
                print(f"‚úÖ Found section: {section}")
            else:
                print(f"‚ùå Missing section: {section}")
                return False
        
        # Check evaluation summary
        summary = data['evaluation_summary']
        if 'scenarios_tested' in summary and 'traits_evaluated' in summary:
            print(f"‚úÖ Evaluation summary complete: {summary['scenarios_tested']} scenarios, {len(summary['traits_evaluated'])} traits")
        else:
            print("‚ùå Evaluation summary incomplete")
            return False
        
        # Check likert evaluation
        likert = data['likert_evaluation']
        if 'agora_original' in likert and 'agora_with_backstory' in likert:
            print("‚úÖ Likert evaluation data complete")
        else:
            print("‚ùå Likert evaluation data incomplete")
            return False
        
        # Check ELO comparison
        elo = data['elo_comparison']
        if 'trait_wins' in elo and 'total_comparisons' in elo:
            print("‚úÖ ELO comparison data complete")
        else:
            print("‚ùå ELO comparison data incomplete")
            return False
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error reading evaluation data: {e}")
        return False

def test_dashboard_imports():
    """Test that dashboard modules can be imported."""
    print("\nüîç Testing dashboard imports...")
    
    try:
        from agora_evaluation_dashboard import AgoraEvaluationDashboard
        print("‚úÖ Successfully imported AgoraEvaluationDashboard")
        
        # Test instantiation
        dashboard = AgoraEvaluationDashboard()
        print("‚úÖ Successfully instantiated dashboard")
        
        # Test file finding
        files = dashboard.find_evaluation_files()
        if files:
            print(f"‚úÖ Found {len(files)} evaluation files")
        else:
            print("‚ùå No evaluation files found")
            return False
        
        # Test data loading
        first_file = files[0]
        summary = dashboard.load_evaluation_summary(first_file['path'])
        if summary:
            print("‚úÖ Successfully loaded evaluation summary")
        else:
            print("‚ùå Failed to load evaluation summary")
            return False
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error importing dashboard: {e}")
        return False

def test_research_logging():
    """Test research logging functionality."""
    print("\nüîç Testing research logging...")
    
    try:
        from research_logger import get_research_logger
        
        logger = get_research_logger()
        print("‚úÖ Successfully created research logger")
        
        # Test logging functionality
        logger.log_prompt_test(
            prompt_type="test",
            prompt_version="v1",
            input_data={"test": "input"},
            output_data={"test": "output"}
        )
        print("‚úÖ Successfully logged test data")
        
        # Test retrieving logs
        logs = logger.get_logs(last_n=1)
        if logs:
            print(f"‚úÖ Successfully retrieved {len(logs)} log entries")
        else:
            print("‚ùå No logs found after logging")
            return False
        
        return True
        
    except Exception as e:
        print(f"‚ùå Error testing research logging: {e}")
        return False

def main():
    """Run all tests."""
    print("üß™ TESTING DASHBOARD FUNCTIONALITY")
    print("=" * 50)
    
    tests = [
        test_sample_data_exists,
        test_evaluation_data_structure,
        test_dashboard_imports,
        test_research_logging
    ]
    
    passed = 0
    failed = 0
    
    for test in tests:
        try:
            if test():
                passed += 1
            else:
                failed += 1
        except Exception as e:
            print(f"‚ùå Test failed with exception: {e}")
            failed += 1
    
    print("\n" + "=" * 50)
    print(f"üìä TEST RESULTS: {passed} passed, {failed} failed")
    
    if failed == 0:
        print("üéâ ALL TESTS PASSED! Dashboard should work correctly.")
    else:
        print("‚ö†Ô∏è  Some tests failed. Please check the issues above.")
    
    return failed == 0

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)