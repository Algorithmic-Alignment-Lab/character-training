import json
from typing import List, Dict, Any
import os
import sys
import asyncio

# Add the project root to the Python path to allow imports from other directories
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))

from conversations_ui.llm_api import call_llm_api

class ConversationGenerator:
    """
    Generates a synthetic conversation between two LLM agents based on character profiles and a scenario.
    """

    def __init__(self, character_profile: Dict[str, Any], model: str):
        """
        Initializes the ConversationGenerator.

        Args:
            character_profile (Dict[str, Any]): A dictionary containing the character's name, archetype, system_prompt, and traits.
            model (str): The name of the language model to use for generation.
        """
        self.character_profile = character_profile
        self.model = model
        self.conversation_log = []

    async def generate_conversation(self, scenario: str, num_turns: int = 5) -> List[Dict[str, Any]]:
        """
        Generates a conversation for a given scenario.

        Args:
            scenario (str): A string describing the situation for the conversation.
            num_turns (int): The number of turns to generate for the conversation.

        Returns:
            A list of dictionaries, where each dictionary represents a message in the conversation.
        """
        self.conversation_log = []
        character_name = self.character_profile.get("name", "Character")
        user_name = "User"

        # Initial message from the user to kick off the conversation
        current_speaker = user_name
        message_content = f"Scenario: {scenario}\n\nAs the user, please start the conversation."
        
        # The first message is just the setup, not part of the history yet.
        # The first turn will be generated by the user model.
        
        history_for_api = [{"role": "system", "content": "You are a person engaging in a conversation based on a scenario. Start the conversation naturally."},{"role": "user", "content": message_content}]

        for i in range(num_turns * 2): 
            if i % 2 == 0:
                current_speaker = user_name
                system_prompt = "You are a user in a conversation. Respond naturally to the character. Keep your responses concise and focused on the scenario."
                model_to_use = "claude-sonnet-4-20250514"
            else:
                current_speaker = character_name
                system_prompt = self.character_profile["system_prompt"]
                model_to_use = "openrouter/qwen/qwen3-32b"

            response_text = await call_llm_api(
                messages=[{"role": "system", "content": system_prompt}] + self._build_message_history(self.conversation_log),
                model=model_to_use,
                temperature=0.7,
                max_tokens=500
            )

            if isinstance(response_text, str) and response_text.startswith("[ERROR:"):
                print(f"Error generating message for {current_speaker}: {response_text}")
                message_content = "[Could not generate a response]"
            else:
                message_content = response_text

            self.conversation_log.append({"speaker": current_speaker, "message": message_content})

        return self.conversation_log

    def _build_message_history(self, log: List[Dict[str, str]]) -> List[Dict[str, str]]:
        """Builds the message history for the LLM API call."""
        messages = []
        for entry in log:
            role = "assistant" if entry["speaker"] != "User" else "user"
            messages.append({"role": role, "content": entry["message"]})
        return messages

    def save_conversation(self, file_path: str):
        """Saves the conversation log to a JSON file."""
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, 'w') as f:
            json.dump(self.conversation_log, f, indent=2)

if __name__ == '__main__':
    # This allows running the script directly for testing
    async def main():
        # Load character definitions
        char_def_path = os.path.join(os.path.dirname(__file__), 'character_definitions.json')
        with open(char_def_path, 'r') as f:
            characters = json.load(f)
        
        jasmine_profile = characters["interviewer_v2_jasmine"]
        
        # Initialize generator
        generator = ConversationGenerator(
            character_profile=jasmine_profile,
            model="claude-3-5-sonnet-20240620"
        )

        # Define scenario and generate conversation
        scenario = "A candidate is being interviewed for a software engineering role. The candidate seems nervous but is clearly passionate about technology."
        conversation = await generator.generate_conversation(scenario, num_turns=4)

        # Print and save the conversation
        print(json.dumps(conversation, indent=2))
        
        output_path = os.path.join(os.path.dirname(__file__), "..", "conversations", "synthetic_conversation_log.json")
        generator.save_conversation(output_path)
        print(f"Conversation saved to {output_path}")

    asyncio.run(main())
